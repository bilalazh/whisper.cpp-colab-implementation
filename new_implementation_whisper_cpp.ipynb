{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaPixy-7wMiu"
      },
      "source": [
        "**Implementing whisper.cpp in google colab for learning purposes**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qVa3nC10h8w"
      },
      "outputs": [],
      "source": [
        "!sudo apt update && sudo apt upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2sI9ldS1O6o"
      },
      "source": [
        "**Install dependencies**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxmaVTVyvrvH"
      },
      "outputs": [],
      "source": [
        "! sudo apt-get install g++ git yt-dlp make\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmlItAme7p3Q"
      },
      "source": [
        "**clone the whisper.cpp git repo**\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qapetrfp9qRt",
        "outputId": "4e5ee0ea-4b51-4d02-87ad-ee47d84d81a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'whisper.cpp'...\n",
            "remote: Enumerating objects: 4897, done.\u001b[K\n",
            "remote: Counting objects: 100% (114/114), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 4897 (delta 38), reused 59 (delta 18), pack-reused 4783\u001b[K\n",
            "Receiving objects: 100% (4897/4897), 8.51 MiB | 19.37 MiB/s, done.\n",
            "Resolving deltas: 100% (3085/3085), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/whisper.cpp.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nx6D6Jo95ZG",
        "outputId": "f4cba46a-148a-48dc-b94b-287353f1cf76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  whisper.cpp\n"
          ]
        }
      ],
      "source": [
        "!ls\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/whisper.cpp/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqOaRXyVggaV",
        "outputId": "51433e19-9a99-4764-c8a4-eb90d7026b15"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/whisper.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-EV_P3QmcAX",
        "outputId": "70cfe2ab-0d70-4f62-a9f4-e3cf3a18ccfa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bindings\texamples      ggml.c\t    ggml-metal.h      ggml-opencl.h  openvino\twhisper.cpp\n",
            "cmake\t\textra\t      ggml-cuda.cu  ggml-metal.m      LICENSE\t     README.md\twhisper.h\n",
            "CMakeLists.txt\tggml-alloc.c  ggml-cuda.h   ggml-metal.metal  Makefile\t     samples\n",
            "coreml\t\tggml-alloc.h  ggml.h\t    ggml-opencl.cpp   models\t     tests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ec2RzNI6jImY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download medium model\n",
        "\n",
        "## here are the codes for all the models\n",
        "**given below are the codes by which the models  are reprsented you may use these codes to make sure to download the right model for you for example**\n",
        "*to download the tiny.en model*\n",
        "\n",
        " edit the command evertwhere like this\n",
        " ```\n",
        " !bash ./models/download-ggml-model.sh tiny.en\n",
        " ```\n",
        "\n",
        "### List of model names\n",
        "\n",
        "models=(\n",
        "\n",
        "    \"tiny.en\"\n",
        "\n",
        "    \"tiny\"\n",
        "\n",
        "    \"tiny-q5_1\"\n",
        "\n",
        "    \"tiny.en-q5_1\"\n",
        "\n",
        "    \"base.en\"\n",
        "\n",
        "    \"base\"\n",
        "\n",
        "    \"base-q5_1\"\n",
        "\n",
        "    \"base.en-q5_1\"\n",
        "\n",
        "    \"small.en\"\n",
        "\n",
        "    \"small.en-tdrz\"\n",
        "\n",
        "    \"small\"\n",
        "\n",
        "\n",
        "    \"small-q5_1\"\n",
        "\n",
        "    \"small.en-q5_1\"\n",
        "\n",
        "    \"medium\"\n",
        "\n",
        "    \"medium.en\"\n",
        "\n",
        "    \"medium-q5_0\"\n",
        "\n",
        "    \"medium.en-q5_0\"\n",
        "\n",
        "    \"large-v1\"\n",
        "\n",
        "    \"large\"\n",
        "\n",
        "    \"large-q5_0\"\n",
        "\n",
        "\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "yxFhWxXVyLPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./models/download-ggml-model.sh tiny.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-o0fLcFmyDO7",
        "outputId": "e89398a8-d9b4-457a-9ca5-102a2413cf57"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ggml model tiny.en from 'https://huggingface.co/ggerganov/whisper.cpp' ...\n",
            "ggml-tiny.en.bin    100%[===================>]  74.10M  22.0MB/s    in 3.4s    \n",
            "Done! Model 'tiny.en' saved in 'models/ggml-tiny.en.bin'\n",
            "You can now use it like this:\n",
            "\n",
            "  $ ./main -m models/ggml-tiny.en.bin -f samples/jfk.wav\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "use it like this\n",
        "```\n",
        "$ ./main -m models/ggml-medium.en-q5_0.bin -f samples/jfk.wav\n",
        "  ```"
      ],
      "metadata": {
        "id": "Z7IH8_nSzl8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "use `!ls` there should be a **Makefile**"
      ],
      "metadata": {
        "id": "sFIojCyWIeCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxMxM_AZ1rsV",
        "outputId": "c4736a71-dab1-4492-af6b-012f6e287ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bench\t\texamples      ggml.c\t    ggml-metal.m      LICENSE\tquantize     whisper.h\n",
            "bindings\textra\t      ggml-cuda.cu  ggml-metal.metal  main\tREADME.md    whisper.o\n",
            "cmake\t\tggml-alloc.c  ggml-cuda.h   ggml.o\t      Makefile\tsamples\n",
            "CMakeLists.txt\tggml-alloc.h  ggml.h\t    ggml-opencl.cpp   models\ttests\n",
            "coreml\t\tggml-alloc.o  ggml-metal.h  ggml-opencl.h     openvino\twhisper.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@"
      ],
      "metadata": {
        "id": "wXks5n2e3k5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Make using Make file**\n"
      ],
      "metadata": {
        "id": "rGXTqifJI5oK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvNtzDAS3hlU",
        "outputId": "cf5366c7-4d4d-4bb0-ea20-89b0e317b339"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I whisper.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3\n",
            "I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:      g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3   -c ggml.c -o ggml.o\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3   -c ggml-alloc.c -o ggml-alloc.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3 -c whisper.cpp -o whisper.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3 examples/main/main.cpp examples/common.cpp examples/common-ggml.cpp ggml.o ggml-alloc.o whisper.o -o main \n",
            "./main -h\n",
            "\n",
            "usage: ./main [options] file0.wav file1.wav ...\n",
            "\n",
            "options:\n",
            "  -h,        --help              [default] show this help message and exit\n",
            "  -t N,      --threads N         [2      ] number of threads to use during computation\n",
            "  -p N,      --processors N      [1      ] number of processors to use during computation\n",
            "  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n",
            "  -on N,     --offset-n N        [0      ] segment index offset\n",
            "  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n",
            "  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n",
            "  -ml N,     --max-len N         [0      ] maximum segment length in characters\n",
            "  -sow,      --split-on-word     [false  ] split on word rather than on token\n",
            "  -bo N,     --best-of N         [2      ] number of best candidates to keep\n",
            "  -bs N,     --beam-size N       [-1     ] beam size for beam search\n",
            "  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n",
            "  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n",
            "  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n",
            "  -debug,    --debug-mode        [false  ] enable debug mode (eg. dump log_mel)\n",
            "  -tr,       --translate         [false  ] translate from source language to english\n",
            "  -di,       --diarize           [false  ] stereo audio diarization\n",
            "  -tdrz,     --tinydiarize       [false  ] enable tinydiarize (requires a tdrz model)\n",
            "  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n",
            "  -otxt,     --output-txt        [false  ] output result in a text file\n",
            "  -ovtt,     --output-vtt        [false  ] output result in a vtt file\n",
            "  -osrt,     --output-srt        [false  ] output result in a srt file\n",
            "  -olrc,     --output-lrc        [false  ] output result in a lrc file\n",
            "  -owts,     --output-words      [false  ] output script for generating karaoke video\n",
            "  -fp,       --font-path         [/System/Library/Fonts/Supplemental/Courier New Bold.ttf] path to a monospace font for karaoke video\n",
            "  -ocsv,     --output-csv        [false  ] output result in a CSV file\n",
            "  -oj,       --output-json       [false  ] output result in a JSON file\n",
            "  -ojf,      --output-json-full  [false  ] include more information in the JSON file\n",
            "  -of FNAME, --output-file FNAME [       ] output file path (without file extension)\n",
            "  -ps,       --print-special     [false  ] print special tokens\n",
            "  -pc,       --print-colors      [false  ] print colors\n",
            "  -pp,       --print-progress    [false  ] print progress\n",
            "  -nt,       --no-timestamps     [false  ] do not print timestamps\n",
            "  -l LANG,   --language LANG     [en     ] spoken language ('auto' for auto-detect)\n",
            "  -dl,       --detect-language   [false  ] exit after automatically detecting language\n",
            "             --prompt PROMPT     [       ] initial prompt\n",
            "  -m FNAME,  --model FNAME       [models/ggml-base.en.bin] model path\n",
            "  -f FNAME,  --file FNAME        [       ] input WAV file path\n",
            "  -oved D,   --ov-e-device DNAME [CPU    ] the OpenVINO device used for encode inference\n",
            "  -ls,       --log-score         [false  ] log best decoder scores of tokens\n",
            "\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3 examples/bench/bench.cpp ggml.o ggml-alloc.o whisper.o -o bench \n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3 examples/quantize/quantize.cpp examples/common.cpp examples/common-ggml.cpp ggml.o ggml-alloc.o whisper.o -o quantize \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There is a short demo called `jfk.wav` included  run this command to run the demo**"
      ],
      "metadata": {
        "id": "Jr9bD0vYJhEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HOW TO USE FLAGS AND OTHER OPTIONS**\n",
        "\n",
        "```\n",
        "usage: ./main [options] file0.wav file1.wav ...\n",
        "\n",
        "options:\n",
        "  -h,        --help              [default] show this help message and exit\n",
        "  -t N,      --threads N         [2      ] number of threads to use during computation\n",
        "  -p N,      --processors N      [1      ] number of processors to use during computation\n",
        "  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n",
        "  -on N,     --offset-n N        [0      ] segment index offset\n",
        "  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n",
        "  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n",
        "  -ml N,     --max-len N         [0      ] maximum segment length in characters\n",
        "  -sow,      --split-on-word     [false  ] split on word rather than on token\n",
        "  -bo N,     --best-of N         [2      ] number of best candidates to keep\n",
        "  -bs N,     --beam-size N       [-1     ] beam size for beam search\n",
        "  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n",
        "  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n",
        "  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n",
        "  -debug,    --debug-mode        [false  ] enable debug mode (eg. dump log_mel)\n",
        "  -tr,       --translate         [false  ] translate from source language to english\n",
        "  -di,       --diarize           [false  ] stereo audio diarization\n",
        "  -tdrz,     --tinydiarize       [false  ] enable tinydiarize (requires a tdrz model)\n",
        "  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n",
        "  -otxt,     --output-txt        [false  ] output result in a text file\n",
        "  -ovtt,     --output-vtt        [false  ] output result in a vtt file\n",
        "  -osrt,     --output-srt        [false  ] output result in a srt file\n",
        "  -olrc,     --output-lrc        [false  ] output result in a lrc file\n",
        "  -owts,     --output-words      [false  ] output script for generating karaoke video\n",
        "  -fp,       --font-path         [/System/Library/Fonts/Supplemental/Courier New Bold.ttf] path to a monospace font for karaoke video\n",
        "  -ocsv,     --output-csv        [false  ] output result in a CSV file\n",
        "  -oj,       --output-json       [false  ] output result in a JSON file\n",
        "  -ojf,      --output-json-full  [false  ] include more information in the JSON file\n",
        "  -of FNAME, --output-file FNAME [       ] output file path (without file extension)\n",
        "  -ps,       --print-special     [false  ] print special tokens\n",
        "  -pc,       --print-colors      [false  ] print colors\n",
        "  -pp,       --print-progress    [false  ] print progress\n",
        "  -nt,       --no-timestamps     [false  ] do not print timestamps\n",
        "  -l LANG,   --language LANG     [en     ] spoken language ('auto' for auto-detect)\n",
        "  -dl,       --detect-language   [false  ] exit after automatically detecting language\n",
        "             --prompt PROMPT     [       ] initial prompt\n",
        "  -m FNAME,  --model FNAME       [models/ggml-base.en.bin] model path\n",
        "  -f FNAME,  --file FNAME        [       ] input WAV file path\n",
        "  -oved D,   --ov-e-device DNAME [CPU    ] the OpenVINO device used for encode inference\n",
        "  -ls,       --log-score         [false  ] log best decoder scores of tokens\n",
        "  ```"
      ],
      "metadata": {
        "id": "7aamJNhVKAmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! ./main -m models/ggml-tiny.en.bin -f samples/jfk.wav\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OJJa1W51lI-",
        "outputId": "fefd2dae-2d43-4dd8-cac7-6478c201f716"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "whisper_init_from_file_no_state: loading model from 'models/ggml-tiny.en.bin'\n",
            "whisper_model_load: loading model\n",
            "whisper_model_load: n_vocab       = 51864\n",
            "whisper_model_load: n_audio_ctx   = 1500\n",
            "whisper_model_load: n_audio_state = 384\n",
            "whisper_model_load: n_audio_head  = 6\n",
            "whisper_model_load: n_audio_layer = 4\n",
            "whisper_model_load: n_text_ctx    = 448\n",
            "whisper_model_load: n_text_state  = 384\n",
            "whisper_model_load: n_text_head   = 6\n",
            "whisper_model_load: n_text_layer  = 4\n",
            "whisper_model_load: n_mels        = 80\n",
            "whisper_model_load: ftype         = 1\n",
            "whisper_model_load: qntvr         = 0\n",
            "whisper_model_load: type          = 1\n",
            "whisper_model_load: adding 1607 extra tokens\n",
            "whisper_model_load: model ctx     =   73.62 MB\n",
            "whisper_model_load: model size    =   73.54 MB\n",
            "whisper_init_state: kv self size  =    2.62 MB\n",
            "whisper_init_state: kv cross size =    8.79 MB\n",
            "whisper_init_state: compute buffer (conv)   =   11.17 MB\n",
            "whisper_init_state: compute buffer (encode) =   61.76 MB\n",
            "whisper_init_state: compute buffer (cross)  =    3.67 MB\n",
            "whisper_init_state: compute buffer (decode) =   18.82 MB\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | COREML = 0 | OPENVINO = 0 | \n",
            "\n",
            "main: processing 'samples/jfk.wav' (176000 samples, 11.0 sec), 2 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n",
            "\n",
            "\n",
            "[00:00:00.000 --> 00:00:08.000]   And so my fellow Americans ask not what your country can do for you\n",
            "[00:00:08.000 --> 00:00:11.000]   ask what you can do for your country.\n",
            "\n",
            "\n",
            "whisper_print_timings:     load time =   124.87 ms\n",
            "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
            "whisper_print_timings:      mel time =    56.78 ms\n",
            "whisper_print_timings:   sample time =    25.48 ms /    27 runs (    0.94 ms per run)\n",
            "whisper_print_timings:   encode time =  2806.82 ms /     1 runs ( 2806.82 ms per run)\n",
            "whisper_print_timings:   decode time =   205.30 ms /    27 runs (    7.60 ms per run)\n",
            "whisper_print_timings:   prompt time =     0.00 ms /     1 runs (    0.00 ms per run)\n",
            "whisper_print_timings:    total time =  3290.42 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WJdKfEkg1mh0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}